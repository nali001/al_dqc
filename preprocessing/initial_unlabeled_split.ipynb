{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the initial and the unlabeled sets for pool-based AL\n",
    "Input: \n",
    "- processed data\n",
    "- n_initial\n",
    "- initial_method\n",
    "\n",
    "Output: \n",
    "- Saved initial and unlabeled sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/randomsplit/train\"\n",
    "\n",
    "initial_method = 'random'\n",
    "# initial_method = 'ocsvm'\n",
    "# initial_method = 'lof'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_ratio(dataset):\n",
    "    ''' Compute anomaly ratio\n",
    "    '''\n",
    "    instance = dataset[(dataset['Label']==1)]\n",
    "    rate=len(instance)/len(dataset)*100\n",
    "    return round(rate,2), len(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(initial_method, csv_file, n_initial, random_seed):\n",
    "    # Load the CSV file into a pandas DataFrame\n",
    "    data_df = pd.read_csv(csv_file)\n",
    "    error_ratio, _ = comp_ratio(data_df)\n",
    "    error_ratio = error_ratio/100\n",
    "\n",
    "    if initial_method=='random': \n",
    "        # Randomly select n_initial samples\n",
    "        initial_set = data_df.sample(n=n_initial, random_state=random_seed)\n",
    "\n",
    "    elif initial_method=='ocsvm':\n",
    "        n_seed = int(n_initial/2)\n",
    "        k = n_initial - n_seed\n",
    "        subset1 = data_df.sample(n=n_seed, random_state=random_seed)\n",
    "        subset2 = data_df.drop(subset1.index)\n",
    "        normal_data = subset1[subset1['Label']==0]\n",
    "        \n",
    "        X = normal_data.drop(columns=['ID', 'Date', 'Label'])  # Replace 'label_column' with the actual label column name\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # Fit the OC-SVM model\n",
    "        ocsvm = OneClassSVM(nu=0.01, kernel='rbf', gamma='scale')  # You can adjust parameters as needed\n",
    "        ocsvm.fit(X_scaled)\n",
    "\n",
    "        Z = subset2.drop(columns=['ID', 'Date', 'Label'])\n",
    "\n",
    "        # Standardize the features\n",
    "        Z_scaled = scaler.fit_transform(Z)\n",
    "\n",
    "        # Predict anomaly scores for instances\n",
    "        anomaly_scores = ocsvm.decision_function(Z_scaled)\n",
    "\n",
    "        # Get the indices of the top k most anomalous instances\n",
    "        top_k_anomalies_indices = anomaly_scores.argsort()[:k]\n",
    "        \n",
    "        # Select the top k most anomalous instances from the original DataFrame\n",
    "        initial_set = pd.concat([subset1, subset2.iloc[top_k_anomalies_indices]])\n",
    "\n",
    "    elif initial_method=='lof': \n",
    "        # Drop any columns that are not features\n",
    "        X = data_df.drop(columns=['ID', 'Date', 'Label'])  # Replace 'label_column' with the actual label column name\n",
    "\n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        # Fit the LOF model\n",
    "        lof = LocalOutlierFactor(n_neighbors=2, contamination=error_ratio)  # You can adjust parameters as needed\n",
    "        labels = lof.fit_predict(X_scaled)\n",
    "\n",
    "        # Get the indices of the top k most anomalous instances\n",
    "        top_k_anomalies_indices = labels.argsort()[:n_initial]\n",
    "\n",
    "        # Select the top k most anomalous instances from the original DataFrame\n",
    "        initial_set = data_df.iloc[top_k_anomalies_indices]\n",
    "\n",
    "    elif initial_method=='iforest': \n",
    "        # Fit the Isolation Forest model\n",
    "        isoforest = IsolationForest(contamination=0.1)  # You can adjust parameters as needed\n",
    "        isoforest.fit(X_scaled)\n",
    "\n",
    "        # Predict anomaly scores (negative scores indicate anomalies)\n",
    "        anomaly_scores = isoforest.decision_function(X_scaled)\n",
    "\n",
    "        # Transform anomaly scores into probability-like values\n",
    "        # Using sigmoid function to map scores to [0, 1]\n",
    "        prob_scores = 1 / (1 + np.exp(-anomaly_scores))\n",
    "\n",
    "        # Get the indices of the top k most anomalous instances\n",
    "        top_k_anomalies_indices = prob_scores.argsort()[:n_initial]\n",
    "\n",
    "        # Select the top k most anomalous instances from the original DataFrame\n",
    "        initial_set = data_df.iloc[top_k_anomalies_indices]\n",
    "\n",
    "    else: \n",
    "        print('Sorry the split method is not supported. (´-ω-`)')\n",
    "        return\n",
    "    \n",
    "    # Get the remaining samples for the unlabeled set\n",
    "    unlabeled_set = data_df.drop(initial_set.index)\n",
    "    \n",
    "    # Get the directory of the input CSV file\n",
    "    csv_dir = os.path.dirname(csv_file)\n",
    "    \n",
    "    # Get the base filename without the extension\n",
    "    base_filename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "    \n",
    "    # Save the initial set and unlabeled set in the same directory\n",
    "    initial_set.to_csv(os.path.join(csv_dir, f'{initial_method}_{base_filename}_{n_initial}_rand_{random_seed}_initial.csv'), index=False)\n",
    "    unlabeled_set.to_csv(os.path.join(csv_dir, f'{initial_method}_{base_filename}_{n_initial}_rand_{random_seed}_unlabeled.csv'), index=False)\n",
    "    \n",
    "    print(f\"{initial_method} split: {error_ratio} errors, {n_initial} initial samples, {initial_set.Label.sum()} anomalies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Float: 4903217 Random seed: 84 ------\n",
      "random split: 0.3372 errors, 100 initial samples, 30 anomalies.\n",
      "random split: 0.3372 errors, 200 initial samples, 72 anomalies.\n",
      "random split: 0.3372 errors, 300 initial samples, 101 anomalies.\n",
      "random split: 0.3372 errors, 400 initial samples, 131 anomalies.\n",
      "------ Float: 4903218 Random seed: 84 ------\n",
      "random split: 0.0084 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0084 errors, 200 initial samples, 3 anomalies.\n",
      "random split: 0.0084 errors, 300 initial samples, 4 anomalies.\n",
      "random split: 0.0084 errors, 400 initial samples, 5 anomalies.\n",
      "------ Float: 4903220 Random seed: 84 ------\n",
      "random split: 0.0016 errors, 100 initial samples, 1 anomalies.\n",
      "random split: 0.0016 errors, 200 initial samples, 1 anomalies.\n",
      "random split: 0.0016 errors, 300 initial samples, 1 anomalies.\n",
      "random split: 0.0016 errors, 400 initial samples, 1 anomalies.\n",
      "------ Float: 4903052 Random seed: 84 ------\n",
      "random split: 0.0069 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0069 errors, 200 initial samples, 2 anomalies.\n",
      "random split: 0.0069 errors, 300 initial samples, 2 anomalies.\n",
      "random split: 0.0069 errors, 400 initial samples, 2 anomalies.\n",
      "------ Float: 4903054 Random seed: 84 ------\n",
      "random split: 0.0023 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 300 initial samples, 1 anomalies.\n",
      "random split: 0.0023 errors, 400 initial samples, 3 anomalies.\n",
      "------ Float: 4903217 Random seed: 65 ------\n",
      "random split: 0.3372 errors, 100 initial samples, 29 anomalies.\n",
      "random split: 0.3372 errors, 200 initial samples, 61 anomalies.\n",
      "random split: 0.3372 errors, 300 initial samples, 99 anomalies.\n",
      "random split: 0.3372 errors, 400 initial samples, 133 anomalies.\n",
      "------ Float: 4903218 Random seed: 65 ------\n",
      "random split: 0.0084 errors, 100 initial samples, 1 anomalies.\n",
      "random split: 0.0084 errors, 200 initial samples, 1 anomalies.\n",
      "random split: 0.0084 errors, 300 initial samples, 4 anomalies.\n",
      "random split: 0.0084 errors, 400 initial samples, 4 anomalies.\n",
      "------ Float: 4903220 Random seed: 65 ------\n",
      "random split: 0.0016 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 300 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 400 initial samples, 0 anomalies.\n",
      "------ Float: 4903052 Random seed: 65 ------\n",
      "random split: 0.0069 errors, 100 initial samples, 3 anomalies.\n",
      "random split: 0.0069 errors, 200 initial samples, 4 anomalies.\n",
      "random split: 0.0069 errors, 300 initial samples, 4 anomalies.\n",
      "random split: 0.0069 errors, 400 initial samples, 4 anomalies.\n",
      "------ Float: 4903054 Random seed: 65 ------\n",
      "random split: 0.0023 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 300 initial samples, 1 anomalies.\n",
      "random split: 0.0023 errors, 400 initial samples, 1 anomalies.\n",
      "------ Float: 4903217 Random seed: 34 ------\n",
      "random split: 0.3372 errors, 100 initial samples, 36 anomalies.\n",
      "random split: 0.3372 errors, 200 initial samples, 73 anomalies.\n",
      "random split: 0.3372 errors, 300 initial samples, 108 anomalies.\n",
      "random split: 0.3372 errors, 400 initial samples, 143 anomalies.\n",
      "------ Float: 4903218 Random seed: 34 ------\n",
      "random split: 0.0084 errors, 100 initial samples, 2 anomalies.\n",
      "random split: 0.0084 errors, 200 initial samples, 6 anomalies.\n",
      "random split: 0.0084 errors, 300 initial samples, 6 anomalies.\n",
      "random split: 0.0084 errors, 400 initial samples, 6 anomalies.\n",
      "------ Float: 4903220 Random seed: 34 ------\n",
      "random split: 0.0016 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 200 initial samples, 1 anomalies.\n",
      "random split: 0.0016 errors, 300 initial samples, 1 anomalies.\n",
      "random split: 0.0016 errors, 400 initial samples, 2 anomalies.\n",
      "------ Float: 4903052 Random seed: 34 ------\n",
      "random split: 0.0069 errors, 100 initial samples, 2 anomalies.\n",
      "random split: 0.0069 errors, 200 initial samples, 3 anomalies.\n",
      "random split: 0.0069 errors, 300 initial samples, 5 anomalies.\n",
      "random split: 0.0069 errors, 400 initial samples, 5 anomalies.\n",
      "------ Float: 4903054 Random seed: 34 ------\n",
      "random split: 0.0023 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 300 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 400 initial samples, 0 anomalies.\n",
      "------ Float: 4903217 Random seed: 25 ------\n",
      "random split: 0.3372 errors, 100 initial samples, 29 anomalies.\n",
      "random split: 0.3372 errors, 200 initial samples, 65 anomalies.\n",
      "random split: 0.3372 errors, 300 initial samples, 98 anomalies.\n",
      "random split: 0.3372 errors, 400 initial samples, 132 anomalies.\n",
      "------ Float: 4903218 Random seed: 25 ------\n",
      "random split: 0.0084 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0084 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0084 errors, 300 initial samples, 0 anomalies.\n",
      "random split: 0.0084 errors, 400 initial samples, 2 anomalies.\n",
      "------ Float: 4903220 Random seed: 25 ------\n",
      "random split: 0.0016 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 300 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 400 initial samples, 0 anomalies.\n",
      "------ Float: 4903052 Random seed: 25 ------\n",
      "random split: 0.0069 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0069 errors, 200 initial samples, 1 anomalies.\n",
      "random split: 0.0069 errors, 300 initial samples, 1 anomalies.\n",
      "random split: 0.0069 errors, 400 initial samples, 2 anomalies.\n",
      "------ Float: 4903054 Random seed: 25 ------\n",
      "random split: 0.0023 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 300 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 400 initial samples, 1 anomalies.\n",
      "------ Float: 4903217 Random seed: 3 ------\n",
      "random split: 0.3372 errors, 100 initial samples, 23 anomalies.\n",
      "random split: 0.3372 errors, 200 initial samples, 57 anomalies.\n",
      "random split: 0.3372 errors, 300 initial samples, 85 anomalies.\n",
      "random split: 0.3372 errors, 400 initial samples, 123 anomalies.\n",
      "------ Float: 4903218 Random seed: 3 ------\n",
      "random split: 0.0084 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0084 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0084 errors, 300 initial samples, 1 anomalies.\n",
      "random split: 0.0084 errors, 400 initial samples, 3 anomalies.\n",
      "------ Float: 4903220 Random seed: 3 ------\n",
      "random split: 0.0016 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 300 initial samples, 0 anomalies.\n",
      "random split: 0.0016 errors, 400 initial samples, 0 anomalies.\n",
      "------ Float: 4903052 Random seed: 3 ------\n",
      "random split: 0.0069 errors, 100 initial samples, 2 anomalies.\n",
      "random split: 0.0069 errors, 200 initial samples, 3 anomalies.\n",
      "random split: 0.0069 errors, 300 initial samples, 5 anomalies.\n",
      "random split: 0.0069 errors, 400 initial samples, 5 anomalies.\n",
      "------ Float: 4903054 Random seed: 3 ------\n",
      "random split: 0.0023 errors, 100 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 200 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 300 initial samples, 0 anomalies.\n",
      "random split: 0.0023 errors, 400 initial samples, 0 anomalies.\n"
     ]
    }
   ],
   "source": [
    "float_numbers = [\n",
    "    '4903217',\n",
    "    '4903218',\n",
    "    '4903220', \n",
    "    '4903052',\n",
    "    '4903054',\n",
    "]\n",
    "\n",
    "float_numbers = [\n",
    "    '4903217',\n",
    "    '4903218',\n",
    "    '4903220', \n",
    "    '4903052',\n",
    "    '4903054',\n",
    "]\n",
    "\n",
    "random_seeds = [84, 65, 34, 25, 3]\n",
    "\n",
    "n_initials = [100, 200, 300, 400]\n",
    "\n",
    "for random_seed in random_seeds: \n",
    "    for float_number in float_numbers: \n",
    "        print(f'------ Float: {float_number} Random seed: {random_seed} ------')\n",
    "        for n_initial in n_initials: \n",
    "            # Specify the CSV file path and the number of initial samples\n",
    "            csv_file = os.path.join(TRAIN_PATH, f'PR_PF_{float_number}.csv')\n",
    "            \n",
    "            # Call the function to split the dataset\n",
    "            split_dataset(initial_method, csv_file, n_initial, random_seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-quality-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
