{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6228a8a8-d1db-4db5-8097-cfedbe148b63",
   "metadata": {},
   "source": [
    "# Supervised learning methods for classification\n",
    "Methods: \n",
    "- Neural networks\n",
    "\n",
    "Compared to classical methods, neural networks take too long to train, and doesn't work well on low-error-rate data, such as on float `4903052`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd60e096-a70f-4de6-9f00-4157352a1489",
   "metadata": {},
   "source": [
    "## Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8b39568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4f26dc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_type = 'random'\n",
    "\n",
    "TRAIN_DIR = f'../data/{split_type}split/train/'\n",
    "VAL_DIR = f'../data/{split_type}split/val/'\n",
    "TEST_DIR = f'../data/{split_type}split/test/'\n",
    "\n",
    "\n",
    "float_numbers = [\n",
    "    '4903052',\n",
    "    '4903054',\n",
    "    '4903058',\n",
    "    '4903215',\n",
    "    '4903217',\n",
    "    '4903218',\n",
    "    '4903220'\n",
    "]\n",
    "\n",
    "# float_number = '4903217' # high\n",
    "# float_number = '4903218' # low1\n",
    "# float_number = '4903220' # low2\n",
    "# float_number = '4903052' # low3\n",
    "float_number = '4903054' # low4\n",
    "\n",
    "\n",
    "TRAIN_FILE = os.path.join(TRAIN_DIR, f'PR_PF_{float_number}.csv')\n",
    "VAL_FILE = os.path.join(VAL_DIR, f'PR_PF_{float_number}.csv')\n",
    "TEST_FILE = os.path.join(TEST_DIR, f'PR_PF_{float_number}.csv')\n",
    "\n",
    "RESULT_DIR = f'../results/{split_type}split/{float_number}_NN'\n",
    "\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35ce2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_ratio(dataset):\n",
    "    ''' Compute anomaly ratio\n",
    "    '''\n",
    "    instance = dataset[(dataset['Label']==1)]\n",
    "    rate=len(instance)/len(dataset)*100\n",
    "    return round(rate,2), len(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07c61b",
   "metadata": {},
   "source": [
    "## Classificatioin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8428e775",
   "metadata": {},
   "source": [
    "### Cost-sensitive learning\n",
    "```\n",
    "              Predicted\n",
    "              '0'   '1'  \n",
    "Actual  '0' |  0  |  1  |\n",
    "        '1' |  10  |  0  |\n",
    "\n",
    "```\n",
    "'1' stands for anomalies. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5e18547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CostSensitiveLoss(nn.Module):\n",
    "#     def __init__(self, cost_matrix):\n",
    "#         super(CostSensitiveLoss, self).__init__()\n",
    "#         self.cost_matrix = cost_matrix\n",
    "        \n",
    "#     def forward(self, outputs, targets):\n",
    "#         # Calculate the loss using the cost matrix and predicted probabilities\n",
    "#         # For example, you can use the cross-entropy loss and apply the cost matrix\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         costs = [self.cost_matrix[int(a), int(b)] for a, b in zip(targets, predicted)]\n",
    "#         costs = torch.tensor(costs)\n",
    "        \n",
    "#         ce_loss = nn.CrossEntropyLoss(reduction='none')(outputs, targets)\n",
    "#         weighted_loss = torch.sum(ce_loss * costs)\n",
    "#         return weighted_loss\n",
    "\n",
    "# cost_matrix = torch.tensor([[1, 1], [10, 1]])  # Example cost matrix\n",
    "# loss_fn = CostSensitiveLoss(cost_matrix)\n",
    "weights = torch.tensor([1, 1], dtype=torch.float)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weights)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6038c4a3-e641-459d-8abd-ddbd5199ae43",
   "metadata": {},
   "source": [
    "### Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b7422a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # out = self.fc2(out)\n",
    "        # out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        # out = self.softmax(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "565209d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 32]             224\n",
      "              ReLU-2                   [-1, 32]               0\n",
      "            Linear-3                    [-1, 2]              66\n",
      "================================================================\n",
      "Total params: 290\n",
      "Trainable params: 290\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = NeuralNetwork(6, 32, 2)\n",
    "summary(model, (6,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889849cd",
   "metadata": {},
   "source": [
    "### Define training and inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76eac3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_nn(input_dim, hidden_dim, output_dim):\n",
    "    model = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "    return model\n",
    "\n",
    "def fit_model_nn(model, criterion, X_train, y_train, X_val=None, y_val=None, epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Create a DataLoader for batch processing\n",
    "    dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    train_loss_values = []\n",
    "    val_loss_values = []\n",
    "    metric_values = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # loss = criterion(outputs, labels)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Calculate the average loss for the epoch\n",
    "        epoch_loss = running_loss / len(X_train)\n",
    "        \n",
    "        # print(f'Epoch: {epoch}; Loss:{epoch_loss}')\n",
    "\n",
    "        # Store the loss value for this epoch\n",
    "        train_loss_values.append(epoch_loss)\n",
    "\n",
    "        # # ------- Validate the results ---------\n",
    "        # if X_val is not None and y_val is not None: \n",
    "        #     # Normalize the features\n",
    "        #     scaler = StandardScaler()\n",
    "        #     X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        #     # Convert numpy arrays to tensors\n",
    "        #     X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "        #     y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)\n",
    "\n",
    "        #     val_outputs = model(X_val_tensor)\n",
    "        #     val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        #     val_loss_values.append(val_loss.item())\n",
    "            \n",
    "        #     metric = list(evaluate_model(model, X_val, y_val, 'NN'))\n",
    "        #     metric_values.append(metric)\n",
    "    return train_loss_values, val_loss_values, metric_values\n",
    "\n",
    "def evaluate_model_nn(model, X_test):\n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_test_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        probabilities = F.softmax(outputs, dim=1).numpy()[:, 1]\n",
    "\n",
    "    return predicted, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f313a850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, input_dim, hidden_dim, output_dim): \n",
    "    if model_name.startswith('NN'):\n",
    "        model = create_model_nn(input_dim, hidden_dim, output_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_features, test_labels):\n",
    "    # Test the model on the test set\n",
    "    predictions, probabilities = evaluate_model_nn(model, test_features)\n",
    "\n",
    "    precision = precision_score(test_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(test_labels, predictions, zero_division=0)\n",
    "    f1 = f1_score(test_labels, predictions, zero_division=0)\n",
    "    kappa = cohen_kappa_score(test_labels, predictions)\n",
    "    roc_auc = roc_auc_score(test_labels, probabilities)\n",
    "    prc_auc = average_precision_score(test_labels, probabilities)\n",
    "    confusion = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "    return precision, recall, f1, kappa, roc_auc, prc_auc, confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42baa963",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3eeb0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(train_data, label_column, sampling_ratio):\n",
    "    # Separate features and labels\n",
    "    train_labels = train_data[label_column]\n",
    "    train_features = train_data.drop(label_column, axis=1)\n",
    "\n",
    "    # Randomly select a subset of the train set\n",
    "    train_features_sample, _, train_labels_sample, _ = train_test_split(train_features, train_labels, train_size=sampling_ratio, random_state=42)\n",
    "\n",
    "    return train_features_sample, train_labels_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "45227c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normalized_date</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Salinity</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59147</th>\n",
       "      <td>-0.751492</td>\n",
       "      <td>1.210129</td>\n",
       "      <td>-1.065537</td>\n",
       "      <td>0.346135</td>\n",
       "      <td>-0.918097</td>\n",
       "      <td>-0.735714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59148</th>\n",
       "      <td>0.733271</td>\n",
       "      <td>0.720008</td>\n",
       "      <td>0.275434</td>\n",
       "      <td>-0.887326</td>\n",
       "      <td>1.013378</td>\n",
       "      <td>0.790247</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59149</th>\n",
       "      <td>0.138937</td>\n",
       "      <td>2.164069</td>\n",
       "      <td>-0.913114</td>\n",
       "      <td>-0.888951</td>\n",
       "      <td>1.126371</td>\n",
       "      <td>1.205522</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59150</th>\n",
       "      <td>-0.391029</td>\n",
       "      <td>0.645212</td>\n",
       "      <td>-0.926159</td>\n",
       "      <td>-0.881200</td>\n",
       "      <td>1.094591</td>\n",
       "      <td>1.124348</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59151</th>\n",
       "      <td>1.685923</td>\n",
       "      <td>-1.635168</td>\n",
       "      <td>2.098484</td>\n",
       "      <td>-0.713296</td>\n",
       "      <td>0.605543</td>\n",
       "      <td>0.518622</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Normalized_date  Latitude  Longitude  Pressure  Salinity  Temperature  \\\n",
       "59147        -0.751492  1.210129  -1.065537  0.346135 -0.918097    -0.735714   \n",
       "59148         0.733271  0.720008   0.275434 -0.887326  1.013378     0.790247   \n",
       "59149         0.138937  2.164069  -0.913114 -0.888951  1.126371     1.205522   \n",
       "59150        -0.391029  0.645212  -0.926159 -0.881200  1.094591     1.124348   \n",
       "59151         1.685923 -1.635168   2.098484 -0.713296  0.605543     0.518622   \n",
       "\n",
       "       Label  \n",
       "59147      0  \n",
       "59148      0  \n",
       "59149      0  \n",
       "59150      0  \n",
       "59151      0  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(TRAIN_FILE)\n",
    "val_data = pd.read_csv(VAL_FILE)\n",
    "test_data = pd.read_csv(TEST_FILE)\n",
    "train_data = train_data.drop(['ID', 'Date'], axis=1)\n",
    "val_data = val_data.drop(['ID', 'Date'], axis=1)\n",
    "test_data = test_data.drop(['ID', 'Date'], axis=1)\n",
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d54595ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((177455, 7), (59152, 7), (59152, 7))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape, val_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db4e2ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tError rate\tAnomalies\n",
      "---------------------------------\n",
      "Train:\t0.23%\t\t400\n",
      "Test:\t0.22%\t\t133\n"
     ]
    }
   ],
   "source": [
    "print(f'\\tError rate\\tAnomalies')\n",
    "print(f'---------------------------------')\n",
    "print(f'Train:\\t{comp_ratio(train_data)[0]}%\\t\\t{comp_ratio(train_data)[1]}'), \n",
    "print(f'Test:\\t{comp_ratio(test_data)[0]}%\\t\\t{comp_ratio(test_data)[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4450f4b9",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8fb3a60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost sensitive weights: [1. 1.]\n",
      "Cost sensitive weights: [1. 2.]\n",
      "Cost sensitive weights: [1. 5.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Sampling Ratio</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "      <th>Cohen's Kappa</th>\n",
       "      <th>ROC-AUC</th>\n",
       "      <th>PRC-AUC</th>\n",
       "      <th>Confusion Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.945504</td>\n",
       "      <td>0.166161</td>\n",
       "      <td>[[59019, 0], [133, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NN+CS2</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.129854</td>\n",
       "      <td>[[59019, 0], [133, 0]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NN+CS5</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.956821</td>\n",
       "      <td>0.226610</td>\n",
       "      <td>[[59019, 0], [133, 0]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  Sampling Ratio  Precision  Recall  F1-score  Cohen's Kappa  \\\n",
       "0      NN            0.99        0.0     0.0       0.0            0.0   \n",
       "1  NN+CS2            0.99        0.0     0.0       0.0            0.0   \n",
       "2  NN+CS5            0.99        0.0     0.0       0.0            0.0   \n",
       "\n",
       "    ROC-AUC   PRC-AUC        Confusion Matrix  \n",
       "0  0.945504  0.166161  [[59019, 0], [133, 0]]  \n",
       "1  0.947849  0.129854  [[59019, 0], [133, 0]]  \n",
       "2  0.956821  0.226610  [[59019, 0], [133, 0]]  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage \n",
    "label_column = 'Label'  # Name of the label column in the CSV files\n",
    "sampling_ratio = 0.99 # Sampling ratio of 0.5 (50%)\n",
    "input_dim = 6\n",
    "hidden_dim = 32\n",
    "output_dim = 2\n",
    "num_epoches = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Perform random sampling\n",
    "train_features_sample, train_labels_sample = random_sampling(train_data, label_column, sampling_ratio)\n",
    "val_labels = val_data[label_column]\n",
    "val_features = val_data.drop(label_column, axis=1)\n",
    "test_labels = test_data[label_column]\n",
    "test_features = test_data.drop(label_column, axis=1)\n",
    "\n",
    "\n",
    "model_names = ['NN', 'NN+CS2', 'NN+CS5']  # Model names to evaluate\n",
    "# model_names = ['NN+CS5']  # Model names to evaluate\n",
    "\n",
    "results = []\n",
    "for model_name in model_names: \n",
    "    # Fit a model on the sampled train set\n",
    "    model = create_model(model_name, input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)\n",
    "    \n",
    "    if model_name == 'NN': \n",
    "        weights = torch.tensor([1, 1], dtype=torch.float)\n",
    "    elif model_name.startswith(\"NN+CS\"): \n",
    "        cs = int(model_name.strip('NN+CS'))\n",
    "        weights = torch.tensor([1, cs], dtype=torch.float)\n",
    "    else: \n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "    \n",
    "    print(f'Cost sensitive weights: {weights.data.numpy()}')\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    train_loss_values, val_loss_values, metric_values = fit_model_nn(model, \n",
    "        criterion, \n",
    "        train_features_sample, \n",
    "        train_labels_sample, \n",
    "        val_features, \n",
    "        val_labels, \n",
    "        epochs=num_epoches, \n",
    "        learning_rate=learning_rate\n",
    "        )\n",
    "    # Evaluate the model on the test set\n",
    "    precision, recall, f1, kappa, roc_auc, prc_auc, confusion = evaluate_model(model, test_features, test_labels)\n",
    "    result = {'Model': model_name, 'Sampling Ratio': sampling_ratio,\n",
    "                                            'Precision': precision, 'Recall': recall, 'F1-score': f1, \"Cohen's Kappa\": kappa, 'ROC-AUC': roc_auc, 'PRC-AUC': prc_auc, 'Confusion Matrix': confusion}\n",
    "\n",
    "    results.append(result)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41922e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float: 4903054\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      " Model &  Precision &  Recall &  F1-score &  Cohen's Kappa &  PRC-AUC \\\\\n",
      "\\midrule\n",
      "    NN &        0.0 &     0.0 &       0.0 &            0.0 &   0.1662 \\\\\n",
      "NN+CS2 &        0.0 &     0.0 &       0.0 &            0.0 &   0.1299 \\\\\n",
      "NN+CS5 &        0.0 &     0.0 &       0.0 &            0.0 &   0.2266 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2273541/417299556.py:11: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  latex_table = filtered_results.to_latex(index=False, escape=False)\n"
     ]
    }
   ],
   "source": [
    "filtered_results = results_df[results_df['Sampling Ratio'] == 0.99]\n",
    "\n",
    "# Select the desired columns\n",
    "selected_columns = ['Model', 'Precision', 'Recall', 'F1-score', \"Cohen's Kappa\", 'PRC-AUC']\n",
    "filtered_results = filtered_results[selected_columns]\n",
    "\n",
    "# Round numerical values to 4 decimals\n",
    "filtered_results = filtered_results.round(4)\n",
    "\n",
    "# Convert the results to LaTeX table format\n",
    "latex_table = filtered_results.to_latex(index=False, escape=False)\n",
    "\n",
    "# Print the LaTeX table\n",
    "print(f\"Float: {float_number}\")\n",
    "\n",
    "print(latex_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
