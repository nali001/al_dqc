{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QBC sampling strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/randomsplit/train\"\n",
    "TEST_PATH = \"../data/randomsplit/test\"\n",
    "\n",
    "float_numbers = [\n",
    "    '4903217',\n",
    "    '4903218',\n",
    "    '4903220', \n",
    "    '4903052',\n",
    "    '4903054',\n",
    "]\n",
    "    \n",
    "float_number = float_numbers[1]\n",
    "\n",
    "\n",
    "# QUERY_STRATEGY = 'random'\n",
    "QUERY_STRATEGY = 'consensus-entropy'\n",
    "RESULT_PATH = f\"../results/randomsplit/{float_number}/{QUERY_STRATEGY}\"\n",
    "\n",
    "import os\n",
    "os.makedirs(RESULT_PATH, exist_ok=True)\n",
    "\n",
    "n_initial = 1000\n",
    "k = 1  # Number of samples to query at each iteration\n",
    "budget = 100  # Number of queried samples desired\n",
    "\n",
    "split_method = 'random'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../results/randomsplit/4903218/consensus-entropy'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_ratio(dataset):\n",
    "    ''' Compute anomaly ratio\n",
    "    '''\n",
    "    instance = dataset[(dataset['Label']==1)]\n",
    "    rate=len(instance)/len(dataset)*100\n",
    "    return round(rate,2), len(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "\n",
    "def create_model(model_name):\n",
    "    if model_name == 'KNN':\n",
    "        model = KNeighborsClassifier(n_neighbors=5, leaf_size=30)\n",
    "    elif model_name == 'LR': \n",
    "        model = LogisticRegression(penalty='l2', random_state=42)\n",
    "    elif model_name == 'RF': \n",
    "        model = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        model = XGBClassifier(max_depth=6)\n",
    "    elif model_name == 'CatBoost':\n",
    "        model = CatBoostClassifier(depth=2, iterations=20, silent=True)\n",
    "    elif model_name == 'LightGBM':\n",
    "        model = LGBMClassifier(max_depth=2, n_estimators=50)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def fit_model(model, labeled_data):\n",
    "    X_train = labeled_data.drop(['ID', 'Label'], axis=1).values\n",
    "    y_train = labeled_data['Label']\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_committee(models, test_data):\n",
    "    X_test = test_data.drop(['ID', 'Label'], axis=1).values\n",
    "    y_test = test_data['Label'].values\n",
    "\n",
    "    model_probabilities = []\n",
    "    for model in models:\n",
    "        model_probabilities.append(model.predict_proba(X_test))\n",
    "\n",
    "    # Compute the mean probability of all models\n",
    "    mean_probabilities = np.mean(model_probabilities, axis=0)\n",
    "    y_pred = np.argmax(mean_probabilities, axis=1)\n",
    "\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    return precision, recall, f1, kappa\n",
    "\n",
    "def query_strategy(strategy_name, models, X_unlabeled, k): \n",
    "    ''' Consensus entropy selects the instance with the maximum entropy in terms of mean probability of the committee. \n",
    "    computation steps: \n",
    "        1. Get predicted class probabilities from each classifier; \n",
    "        2. Calculate the mean probability of the committee; \n",
    "        3. Calculate the entropy of mean probabilities; \n",
    "        4. Select instances with highest entropy. \n",
    "    Ref: https://modal-python.readthedocs.io/en/latest/content/query_strategies/Disagreement-sampling.html\n",
    "    '''\n",
    "    \n",
    "    if strategy_name == 'consensus-entropy': \n",
    "        model_probabilities = []\n",
    "        for model in models:\n",
    "            model_probabilities.append(model.predict_proba(X_unlabeled))\n",
    "        # Compute the mean probability of all models\n",
    "        mean_probabilities = np.mean(model_probabilities, axis=0)\n",
    "\n",
    "        consensus_entropy = -np.sum(np.where(mean_probabilities != 0, mean_probabilities * np.log2(mean_probabilities), 0), axis=1)\n",
    "        query_indices = np.argsort(consensus_entropy)[-k:]\n",
    "       \n",
    "    elif strategy_name == 'max-disagreement': \n",
    "        num_rows = len(X_unlabeled)\n",
    "        if num_rows == 0:\n",
    "            raise ValueError(\"The matrix is empty.\")\n",
    "        if k > num_rows:\n",
    "            raise ValueError(\"The number of rows to select is greater than the number of rows in the matrix.\")\n",
    "        query_indices = random.sample(range(num_rows), k)\n",
    "    \n",
    "    return query_indices\n",
    "\n",
    "def qbc(model_names, initial_data, unlabeled_data, test_data, k, budget):\n",
    "    models = [create_model(model_name=model_name) for model_name in model_names]\n",
    "    \n",
    "    labeled_data = initial_data.copy()  # Initialize the labeled set with the initial data\n",
    "\n",
    "    queried_samples = 0\n",
    "    query_indices = []\n",
    "    query_ids = []\n",
    "\n",
    "    metrics = {\n",
    "        # 'model_name': model_name, \n",
    "        'num_samples': [], \n",
    "        'query_ids': [], \n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-score': [],\n",
    "        'Kappa': []\n",
    "        }\n",
    "    while queried_samples <= budget:\n",
    "        # Train the models on the initial data\n",
    "        trained_models = []\n",
    "        for model in models: \n",
    "            model = fit_model(model, labeled_data)\n",
    "            trained_models.append(model)\n",
    "        \n",
    "        # Evaluate the committee\n",
    "        precision, recall, f1, kappa = evaluate_committee(trained_models, test_data)\n",
    "\n",
    "        # Store the metrics for the current model\n",
    "        metrics['num_samples'].append(queried_samples)\n",
    "        metrics['query_ids'].append(query_ids)\n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1-score'].append(f1)\n",
    "        metrics['Kappa'].append(kappa)\n",
    "        \n",
    "        # Compute uncertainty scores for the remaining unlabeled set\n",
    "        X_unlabeled = unlabeled_data.drop(['ID', 'Label'], axis=1).values\n",
    "        query_indices = query_strategy(QUERY_STRATEGY, trained_models, X_unlabeled, k)\n",
    "        \n",
    "        # Add the queried samples to the labeled set\n",
    "        labeled_data = pd.concat([labeled_data, unlabeled_data.iloc[query_indices]])\n",
    "        query_ids = unlabeled_data.iloc[query_indices]['ID'].to_list()\n",
    "\n",
    "        print(f\"ID: {unlabeled_data.iloc[query_indices]['ID'].to_list()}; Label: {unlabeled_data.iloc[query_indices]['Label'].to_list()}\")\n",
    "\n",
    "        # Remove the queried samples from the unlabeled set\n",
    "        unlabeled_data = unlabeled_data.drop(unlabeled_data.index[query_indices])\n",
    "\n",
    "        # Update the number of queried samples\n",
    "        queried_samples += len(query_indices)\n",
    "\n",
    "        # # Train the final model on the labeled set\n",
    "        # model = fit_model(model, labeled_data)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Example usage\n",
    "import os \n",
    "train_file = os.path.join(TRAIN_PATH, f'PR_PF_{float_number}.csv')\n",
    "test_file = os.path.join(TEST_PATH, f'PR_PF_{float_number}.csv')\n",
    "initial_file = os.path.join(TRAIN_PATH, f'{split_method}_PR_PF_{float_number}_{n_initial}_initial.csv')\n",
    "unlabeled_file = os.path.join(TRAIN_PATH, f'{split_method}_PR_PF_{float_number}_{n_initial}_unlabeled.csv')\n",
    "\n",
    "\n",
    "# Load the train and test datasets\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "initial_data = pd.read_csv(initial_file)\n",
    "unlabeled_data = pd.read_csv(unlabeled_file)\n",
    "\n",
    "train_data = train_data.drop('Date', axis=1)\n",
    "test_data = test_data.drop('Date', axis=1)\n",
    "initial_data = initial_data.drop('Date', axis=1)\n",
    "unlabeled_data = unlabeled_data.drop('Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['KNN', 'XGBoost', 'CatBoost', 'LightGBM']\n"
     ]
    }
   ],
   "source": [
    "def generate_subsets(input_list, n_elements):\n",
    "    n = len(input_list)\n",
    "    subsets = []\n",
    "\n",
    "    for i in range(2 ** n):\n",
    "        subset = [input_list[j] for j in range(n) if (i & (1 << j)) > 0]\n",
    "        if len(subset) == n_elements:\n",
    "            subsets.append(subset)\n",
    "\n",
    "    return subsets\n",
    "\n",
    "elements = ['KNN', 'XGBoost', 'CatBoost', 'LightGBM']\n",
    "\n",
    "combinations_2 = generate_subsets(elements, 2)\n",
    "combinations_3 = generate_subsets(elements, 3)\n",
    "combinations_4 = generate_subsets(elements, 4)\n",
    "for item in combinations_4: \n",
    "    print(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: [107561]; Label: [1]\n",
      "ID: [106445]; Label: [1]\n",
      "ID: [106522]; Label: [1]\n",
      "ID: [106436]; Label: [1]\n",
      "ID: [106427]; Label: [1]\n",
      "ID: [107762]; Label: [1]\n",
      "ID: [106309]; Label: [1]\n",
      "ID: [107738]; Label: [1]\n",
      "ID: [107291]; Label: [1]\n",
      "ID: [107741]; Label: [1]\n",
      "ID: [106622]; Label: [1]\n",
      "ID: [107887]; Label: [1]\n",
      "ID: [106649]; Label: [1]\n",
      "ID: [107895]; Label: [1]\n",
      "ID: [107920]; Label: [1]\n",
      "ID: [106417]; Label: [1]\n",
      "ID: [106442]; Label: [1]\n",
      "ID: [108005]; Label: [1]\n",
      "ID: [107994]; Label: [1]\n",
      "ID: [106366]; Label: [1]\n",
      "ID: [106347]; Label: [1]\n",
      "ID: [106388]; Label: [1]\n",
      "ID: [106393]; Label: [1]\n",
      "ID: [93653]; Label: [0]\n",
      "ID: [106392]; Label: [1]\n",
      "ID: [104449]; Label: [0]\n",
      "ID: [106413]; Label: [1]\n",
      "ID: [106665]; Label: [1]\n",
      "ID: [106409]; Label: [1]\n",
      "ID: [104488]; Label: [0]\n",
      "ID: [106339]; Label: [1]\n",
      "ID: [106692]; Label: [1]\n",
      "ID: [106742]; Label: [0]\n",
      "ID: [106340]; Label: [1]\n",
      "ID: [106731]; Label: [0]\n",
      "ID: [106718]; Label: [1]\n",
      "ID: [106764]; Label: [0]\n",
      "ID: [106716]; Label: [1]\n",
      "ID: [106740]; Label: [0]\n",
      "ID: [106719]; Label: [1]\n",
      "ID: [108419]; Label: [0]\n",
      "ID: [106735]; Label: [0]\n",
      "ID: [106726]; Label: [1]\n",
      "ID: [108446]; Label: [0]\n",
      "ID: [102985]; Label: [0]\n",
      "ID: [104783]; Label: [0]\n",
      "ID: [106407]; Label: [1]\n",
      "ID: [106674]; Label: [1]\n",
      "ID: [104787]; Label: [0]\n",
      "ID: [106728]; Label: [0]\n",
      "ID: [91850]; Label: [0]\n",
      "ID: [127858]; Label: [0]\n",
      "ID: [106363]; Label: [1]\n",
      "ID: [104443]; Label: [0]\n",
      "ID: [104419]; Label: [0]\n",
      "ID: [106341]; Label: [1]\n",
      "ID: [108449]; Label: [0]\n",
      "ID: [106724]; Label: [1]\n",
      "ID: [104797]; Label: [0]\n",
      "ID: [106351]; Label: [1]\n",
      "ID: [104417]; Label: [0]\n",
      "ID: [106725]; Label: [1]\n",
      "ID: [106337]; Label: [1]\n",
      "ID: [110011]; Label: [0]\n",
      "ID: [104431]; Label: [0]\n",
      "ID: [107998]; Label: [1]\n",
      "ID: [106379]; Label: [1]\n",
      "ID: [127872]; Label: [0]\n",
      "ID: [108099]; Label: [0]\n",
      "ID: [106382]; Label: [1]\n",
      "ID: [104461]; Label: [0]\n",
      "ID: [106380]; Label: [1]\n",
      "ID: [108119]; Label: [0]\n",
      "ID: [110030]; Label: [0]\n",
      "ID: [111833]; Label: [0]\n",
      "ID: [107992]; Label: [1]\n",
      "ID: [106678]; Label: [1]\n",
      "ID: [108404]; Label: [0]\n",
      "ID: [106333]; Label: [1]\n",
      "ID: [104428]; Label: [0]\n",
      "ID: [111840]; Label: [0]\n",
      "ID: [131358]; Label: [0]\n",
      "ID: [110375]; Label: [0]\n",
      "ID: [131376]; Label: [0]\n",
      "ID: [108117]; Label: [0]\n",
      "ID: [106386]; Label: [1]\n",
      "ID: [104471]; Label: [0]\n",
      "ID: [108131]; Label: [0]\n",
      "ID: [106400]; Label: [1]\n",
      "ID: [106404]; Label: [1]\n",
      "ID: [104495]; Label: [0]\n",
      "ID: [106353]; Label: [1]\n",
      "ID: [129611]; Label: [0]\n",
      "ID: [108082]; Label: [0]\n",
      "ID: [104415]; Label: [0]\n",
      "ID: [106336]; Label: [1]\n",
      "ID: [104481]; Label: [0]\n",
      "ID: [106396]; Label: [1]\n",
      "ID: [108139]; Label: [0]\n",
      "ID: [104444]; Label: [0]\n",
      "ID: [104472]; Label: [0]\n",
      "Save to ../results/randomsplit/4903218/consensus-entropy/KNN+CatBoost_random_1000_initial_1_k.csv\n"
     ]
    }
   ],
   "source": [
    "# model_names = ['XGBoost'] Y\n",
    "\n",
    "# model_names = ['KNN', 'XGBoost', 'CatBoost', 'LightGBM'] Y\n",
    "\n",
    "# model_names = ['KNN', 'XGBoost', 'CatBoost']\n",
    "# model_names = ['KNN', 'XGBoost', 'LightGBM'] N\n",
    "# model_names = ['KNN', 'CatBoost', 'LightGBM'] N\n",
    "# model_names = ['XGBoost', 'CatBoost', 'LightGBM'] Y\n",
    "\n",
    "# model_names = ['XGBoost', 'CatBoost', 'LightGBM'] Y\n",
    "\n",
    "combinations = [\n",
    "                ['KNN', 'CatBoost'], \n",
    "                # ['XGBoost', 'CatBoost'], \n",
    "                # ['KNN', 'XGBoost', 'CatBoost'], \n",
    "                # ['KNN', 'XGBoost', 'CatBoost', 'LightGBM'],\n",
    "    ]\n",
    "\n",
    "for model_names in combinations: \n",
    "    # Dictionary to store the evaluation metrics for each model\n",
    "    metrics = {}\n",
    "\n",
    "    # Active learning loop\n",
    "    metrics = qbc(model_names, initial_data, unlabeled_data, test_data, k, budget)\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    filename = f\"{RESULT_PATH}/{'+'.join(model_names)}_{split_method}_{n_initial}_initial_{k}_k.csv\"\n",
    "    df_metrics.to_csv(filename, index=False)\n",
    "    print(f\"Save to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-quality-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
