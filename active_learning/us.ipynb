{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncertainty-based and random sampling strategies\n",
    "This script run multiple classifiers in AL. You can specify the query strategy and float being used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../data/randomsplit/train\"\n",
    "TEST_PATH = \"../data/randomsplit/test\"\n",
    "\n",
    "float_numbers = [\n",
    "    '4903217',\n",
    "    '4903218',\n",
    "    '4903220', \n",
    "    '4903052',\n",
    "    '4903054',\n",
    "]\n",
    "    \n",
    "float_number = float_numbers[1]\n",
    "\n",
    "QUERY_STRATEGY = 'random'\n",
    "# QUERY_STRATEGY = 'uncertainty'\n",
    "\n",
    "RESULT_PATH = f\"../results/randomsplit/{float_number}/{QUERY_STRATEGY}\"\n",
    "\n",
    "import os\n",
    "os.makedirs(RESULT_PATH, exist_ok=True)\n",
    "\n",
    "n_initial = 1000\n",
    "k = 1  # Number of samples to query at each iteration\n",
    "budget = 100  # Number of queried samples desired\n",
    "\n",
    "split_method = 'random'\n",
    "# split_method = 'ocsvm'\n",
    "# split_method = 'lof'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_ratio(dataset):\n",
    "    ''' Compute anomaly ratio\n",
    "    '''\n",
    "    instance = dataset[(dataset['Label']==1)]\n",
    "    rate=len(instance)/len(dataset)*100\n",
    "    return round(rate,2), len(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, cohen_kappa_score\n",
    "\n",
    "from modAL.uncertainty import uncertainty_sampling\n",
    "\n",
    "def create_model(model_name): \n",
    "    if model_name == 'KNN':\n",
    "        model = KNeighborsClassifier(n_neighbors=5, leaf_size=30)\n",
    "        model = RandomForestClassifier(n_estimators=20, random_state=42)\n",
    "    elif model_name == 'XGBoost':\n",
    "        model = XGBClassifier(max_depth=6)\n",
    "    elif model_name == 'CatBoost':\n",
    "        model = CatBoostClassifier(depth=2, iterations=20, silent=True)\n",
    "    elif model_name == 'LightGBM':\n",
    "        model = LGBMClassifier(max_depth=2, n_estimators=50)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_model(model, labeled_data, model_name):\n",
    "    X_train = labeled_data.drop(['ID', 'Label'], axis=1).values\n",
    "    y_train = labeled_data['Label']\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, test_data, model_name):\n",
    "    X_test = test_data.drop(['ID', 'Label'], axis=1).values\n",
    "    y_test = test_data['Label'].values\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    return precision, recall, f1, kappa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define AL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_strategy(strategy_name, model, X_unlabeled, k, model_name): \n",
    "    if strategy_name == 'uncertainty': \n",
    "        probabilities = model.predict_proba(X_unlabeled)\n",
    "        # uncertainty_scores = uncertainty_sampling(model, X_unlabeled, n_instances=k)\n",
    "        uncertainty_scores = 1 - (probabilities.max(axis=1))\n",
    "         # Select the top-k most uncertain samples\n",
    "        query_indices = (-uncertainty_scores).argsort()[:k]\n",
    "    elif strategy_name == 'random': \n",
    "        num_rows = len(X_unlabeled)\n",
    "        if num_rows == 0:\n",
    "            raise ValueError(\"The matrix is empty.\")\n",
    "        if k > num_rows:\n",
    "            raise ValueError(\"The number of rows to select is greater than the number of rows in the matrix.\")\n",
    "        query_indices = random.sample(range(num_rows), k)\n",
    "    \n",
    "    return query_indices\n",
    "\n",
    "def pool_based_active_learning(model_name, initial_data, unlabeled_data, test_data, k, budget):\n",
    "    model = create_model(model_name=model_name)\n",
    "    labeled_data = initial_data.copy()  # Initialize the labeled set with the initial data\n",
    "\n",
    "    queried_samples = 0\n",
    "    query_indices = []\n",
    "    query_ids = []\n",
    "\n",
    "    # model_name = model.__class__.__name__\n",
    "    metrics = {\n",
    "        # 'model_name': model_name, \n",
    "        'num_samples': [], \n",
    "        'query_ids': [], \n",
    "        'Precision': [],\n",
    "        'Recall': [],\n",
    "        'F1-score': [],\n",
    "        'Kappa': []\n",
    "        }\n",
    "    while queried_samples <= budget:\n",
    "        # Iterate over the models\n",
    "        # Train the model on the initial data\n",
    "        model = fit_model(model, labeled_data, model_name)\n",
    "        \n",
    "        # Evaluate the model\n",
    "        precision, recall, f1, kappa = evaluate_model(model, test_data, model_name)\n",
    "\n",
    "        # Store the metrics for the current model\n",
    "        metrics['num_samples'].append(queried_samples)\n",
    "        metrics['query_ids'].append(query_ids)\n",
    "        metrics['Precision'].append(precision)\n",
    "        metrics['Recall'].append(recall)\n",
    "        metrics['F1-score'].append(f1)\n",
    "        metrics['Kappa'].append(kappa)\n",
    "        \n",
    "        # Compute uncertainty scores for the remaining unlabeled set\n",
    "        # uncertainty_scores = -model.predict_proba(unlabeled_data.drop(['ID', 'Label'], axis=1).values).max(axis=1)\n",
    "        X_unlabeled = unlabeled_data.drop(['ID', 'Label'], axis=1).values\n",
    "        query_indices = query_strategy(QUERY_STRATEGY, model, X_unlabeled, k, model_name)\n",
    "\n",
    "        # Add the queried samples to the labeled set\n",
    "        # labeled_data = np.concatenate((labeled_data, unlabeled_data.iloc[query_indices]))\n",
    "        labeled_data = pd.concat([labeled_data, unlabeled_data.iloc[query_indices]])\n",
    "        query_ids = unlabeled_data.iloc[query_indices]['ID'].to_list()\n",
    "\n",
    "        print(f\"# samples: {queried_samples}; ID: {unlabeled_data.iloc[query_indices]['ID'].to_list()}; Label: {unlabeled_data.iloc[query_indices]['Label'].to_list()}\")\n",
    "\n",
    "        # Remove the queried samples from the unlabeled set\n",
    "        unlabeled_data = unlabeled_data.drop(unlabeled_data.index[query_indices])\n",
    "\n",
    "        # Update the number of queried samples\n",
    "        queried_samples += len(query_indices)\n",
    "\n",
    "        # # Train the final model on the labeled set\n",
    "        # model = fit_model(model, labeled_data)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# Example usage\n",
    "import os \n",
    "train_file = os.path.join(TRAIN_PATH, f'PR_PF_{float_number}.csv')\n",
    "test_file = os.path.join(TEST_PATH, f'PR_PF_{float_number}.csv')\n",
    "initial_file = os.path.join(TRAIN_PATH, f'{split_method}_PR_PF_{float_number}_{n_initial}_initial.csv')\n",
    "unlabeled_file = os.path.join(TRAIN_PATH, f'{split_method}_PR_PF_{float_number}_{n_initial}_unlabeled.csv')\n",
    "\n",
    "# Load the train and test datasets\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "initial_data = pd.read_csv(initial_file)\n",
    "unlabeled_data = pd.read_csv(unlabeled_file)\n",
    "\n",
    "train_data = train_data.drop('Date', axis=1)\n",
    "test_data = test_data.drop('Date', axis=1)\n",
    "initial_data = initial_data.drop('Date', axis=1)\n",
    "unlabeled_data = unlabeled_data.drop('Date', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'------- {float_number} ------')\n",
    "print(f'Train: {train_data.shape[0]}; {comp_ratio(train_data)[0]}%')\n",
    "print(f'Test: {test_data.shape[0]}; {comp_ratio(test_data)[0]}%')\n",
    "print(f\"Initial: {n_initial}; {initial_data.Label.sum()} anomalies.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start AL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['KNN', 'XGBoost', 'CatBoost', 'LightGBM']\n",
    "\n",
    "# Dictionary to store the evaluation metrics for each model\n",
    "metrics = {}\n",
    "\n",
    "# Active learning loop\n",
    "for model_name in model_names:\n",
    "    metrics = pool_based_active_learning(model_name, initial_data, unlabeled_data, test_data, k, budget)\n",
    "    df_metrics = pd.DataFrame(metrics)\n",
    "    filename = f\"{RESULT_PATH}/{model_name}_{split_method}_{n_initial}_initial_{k}_k.csv\"\n",
    "    df_metrics.to_csv(filename, index=False)\n",
    "    print(f\"Save to {filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-quality-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
